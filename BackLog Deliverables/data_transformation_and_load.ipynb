{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation and Loading Backlog\n",
    "\n",
    "**Goal:** \n",
    " * To transform our data by removing unecessary features as well as converting data types and loading the dataset into a platform easily accessible for our analysts to perform their analysis.\n",
    "\n",
    "**Solution:**\n",
    " * AWS Glue ETL Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Steps Executed in Extraction:\n",
    "\n",
    "- During our extraction phase we did some transformation with AWS lambdas we used to source our data. The lambdas get rid of duplicate entries and flatten our JSON data into a CSV file format.\n",
    "- The data was flattened in Lambda because we were not able to succesfully flatten the data using AWS Glue but the result is that we saved in computing costs that are much higher for ETL jobs in AWS Glue than in AWS Lambda.\n",
    "\n",
    "##### Extraction script used prior to transformation:\n",
    "*(For demonstration purposes the script is shown without AWS libraries. The script including AWS libraries can be found in the `extraction.ipynb` backlog)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5458 entries, 0 to 5479\n",
      "Data columns (total 29 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   job_id                         5458 non-null   object\n",
      " 1   agency                         5458 non-null   object\n",
      " 2   posting_type                   5458 non-null   object\n",
      " 3   number_of_positions            5458 non-null   object\n",
      " 4   business_title                 5458 non-null   object\n",
      " 5   civil_service_title            5458 non-null   object\n",
      " 6   title_classification           5458 non-null   object\n",
      " 7   title_code_no                  5458 non-null   object\n",
      " 8   level                          5458 non-null   object\n",
      " 9   job_category                   5458 non-null   object\n",
      " 10  full_time_part_time_indicator  5338 non-null   object\n",
      " 11  career_level                   5458 non-null   object\n",
      " 12  salary_range_from              5458 non-null   object\n",
      " 13  salary_range_to                5458 non-null   object\n",
      " 14  salary_frequency               5458 non-null   object\n",
      " 15  work_location                  5458 non-null   object\n",
      " 16  division_work_unit             5458 non-null   object\n",
      " 17  job_description                5458 non-null   object\n",
      " 18  minimum_qual_requirements      5415 non-null   object\n",
      " 19  preferred_skills               4165 non-null   object\n",
      " 20  to_apply                       4945 non-null   object\n",
      " 21  residency_requirement          5458 non-null   object\n",
      " 22  posting_date                   5458 non-null   object\n",
      " 23  post_until                     1788 non-null   object\n",
      " 24  posting_updated                5458 non-null   object\n",
      " 25  process_date                   5458 non-null   object\n",
      " 26  additional_information         3432 non-null   object\n",
      " 27  hours_shift                    1978 non-null   object\n",
      " 28  work_location_1                1963 non-null   object\n",
      "dtypes: object(29)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'https://data.cityofnewyork.us/resource/kpav-sd4t.json'\n",
    "\n",
    "\n",
    "params = {'$limit': 6000}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "\n",
    "df_main = pd.DataFrame(data)\n",
    "df_main.drop_duplicates(inplace=True)\n",
    "\n",
    "df_main.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming & Loading Features:\n",
    "\n",
    "#### Feature Selection & Data Types\n",
    "- After our extraction phase we're left with a dataset that includes 29 different features and all of the features are of string type.\n",
    "- So we needed to get rid of unnecessary features and convert features leftover to their correct data type.\n",
    "- To find features needed to be removed and the correct data types we conducted a data analysis on our local machines. The data analysis can be found in the main directory python script `cleaning_code_FULL.py`.\n",
    "\n",
    "#### Configuring AWS Services:\n",
    "- After we selected the features we will be using and their datatypes needed we were ready to start our AWS Glue ETL Job.\n",
    "- In order for our ETL Job to work we had to setup and configure the following services in our AWS console:\n",
    "    1. Redshift Namespace and Workgroup:\n",
    "       * The platform our transfomred data is loaded to.\n",
    "    3. IAM Role for AWS Glue permissions:\n",
    "       * Permission for Glue to have read access to our extracted data stored in our S3 bucket.\n",
    "       * Permission to access redshift data to execute query statements\n",
    "       * Permission to access the VPC where our services were run.\n",
    "    4. AWS Glue Connector:\n",
    "       * We needed to create a connector in order to connect to our Redshift database.\n",
    "    5. A route table for our VPC's security group.\n",
    "- Our biggest challenge was trying to connect without having the proper route table set up for our VPC's security group. We were not able to succesfully run our ETL Job until we discovered that this was the missing link.\n",
    "\n",
    "\n",
    "#### Creating & Running ETL Job:\n",
    "- After everything was correctly configured transforming and loading our data was relatively easy.\n",
    "- We ended up using Visual ETL in AWS Glue ETL Jobs which allows you to construct a blueprint for your pipeline by connecting and syncing to your configured services in real time.\n",
    "- After the blueprint is constructed the script for running the Job is created and we just had to run our Job to transform and load our data.\n",
    "\n",
    "#### ETL Job Blueprints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layout of ETL Job:\n",
    "![Glue Blueprint](./static/glueprint.png)\n",
    "## Loading Data From S3:\n",
    "![ETL 1](./static/ETL1.png)\n",
    "## Dropping Tables and Converting Datatypes in Schema:\n",
    "![ETL 2](./static/ETL2.png)\n",
    "## Creating/Updating Table in our Redshift Database:\n",
    "![ETL 3](./static/ETL3.png)\n",
    "## Confirming our Job has Run Successfully:\n",
    "![ETL 4](./static/ETL4.png)\n",
    "## Querying our Data in Redshift:\n",
    "![transformed data](./static/transformed_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final ETL Job Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from awsglue.transforms import *\n",
    "# from awsglue.utils import getResolvedOptions\n",
    "# from pyspark.context import SparkContext\n",
    "# from awsglue.context import GlueContext\n",
    "# from awsglue.job import Job\n",
    "# from awsglue import DynamicFrame\n",
    "\n",
    "# args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "# sc = SparkContext()\n",
    "# glueContext = GlueContext(sc)\n",
    "# spark = glueContext.spark_session\n",
    "# job = Job(glueContext)\n",
    "# job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# # Script generated for node Amazon S3\n",
    "# AmazonS3_node1708543405168 = glueContext.create_dynamic_frame.from_options(\n",
    "#     format_options={\n",
    "#         \"quoteChar\": '\"',\n",
    "#         \"withHeader\": True,\n",
    "#         \"separator\": \",\",\n",
    "#         \"optimizePerformance\": False,\n",
    "#     },\n",
    "#     connection_type=\"s3\",\n",
    "#     format=\"csv\",\n",
    "#     connection_options={\"paths\": [\"s3://semistructuredata\"]},\n",
    "#     transformation_ctx=\"AmazonS3_node1708543405168\",\n",
    "# )\n",
    "\n",
    "# # Script generated for node Change Schema\n",
    "# ChangeSchema_node1708543471726 = ApplyMapping.apply(\n",
    "#     frame=AmazonS3_node1708543405168,\n",
    "#     mappings=[\n",
    "#         (\"job_id\", \"string\", \"job_id\", \"string\"),\n",
    "#         (\"agency\", \"string\", \"agency\", \"string\"),\n",
    "#         (\"posting_type\", \"string\", \"posting_type\", \"string\"),\n",
    "#         (\"number_of_positions\", \"string\", \"number_of_positions\", \"int\"),\n",
    "#         (\"business_title\", \"string\", \"business_title\", \"string\"),\n",
    "#         (\"civil_service_title\", \"string\", \"civil_service_title\", \"string\"),\n",
    "#         (\"level\", \"string\", \"level\", \"string\"),\n",
    "#         (\n",
    "#             \"full_time_part_time_indicator\",\n",
    "#             \"string\",\n",
    "#             \"full_time_part_time_indicator\",\n",
    "#             \"string\",\n",
    "#         ),\n",
    "#         (\"career_level\", \"string\", \"career_level\", \"string\"),\n",
    "#         (\"salary_range_from\", \"string\", \"salary_range_from\", \"decimal\"),\n",
    "#         (\"salary_range_to\", \"string\", \"salary_range_to\", \"decimal\"),\n",
    "#         (\"salary_frequency\", \"string\", \"salary_frequency\", \"string\"),\n",
    "#         (\"work_location\", \"string\", \"work_location\", \"string\"),\n",
    "#         (\"division_work_unit\", \"string\", \"division_work_unit\", \"string\"),\n",
    "#         (\"posting_date\", \"string\", \"posting_date\", \"date\"),\n",
    "#         (\"post_until\", \"string\", \"post_until\", \"string\"),\n",
    "#     ],\n",
    "#     transformation_ctx=\"ChangeSchema_node1708543471726\",\n",
    "# )\n",
    "\n",
    "# # Script generated for node Amazon Redshift\n",
    "# AmazonRedshift_node1708544284158 = glueContext.write_dynamic_frame.from_options(\n",
    "#     frame=ChangeSchema_node1708543471726,\n",
    "#     connection_type=\"redshift\",\n",
    "#     connection_options={\n",
    "#         \"redshiftTmpDir\": \"s3://aws-glue-assets-637423487153-us-east-1/temporary/\",\n",
    "#         \"useConnectionProperties\": \"true\",\n",
    "#         \"dbtable\": \"public.joblistings\",\n",
    "#         \"connectionName\": \"Redshift nyc listings connect\",\n",
    "#         \"preactions\": \"DROP TABLE IF EXISTS public.joblistings; CREATE TABLE IF NOT EXISTS public.joblistings (job_id VARCHAR, agency VARCHAR, posting_type VARCHAR, number_of_positions INTEGER, business_title VARCHAR, civil_service_title VARCHAR, level VARCHAR, full_time_part_time_indicator VARCHAR, career_level VARCHAR, salary_range_from DECIMAL, salary_range_to DECIMAL, salary_frequency VARCHAR, work_location VARCHAR, division_work_unit VARCHAR, posting_date DATE, post_until VARCHAR);\",\n",
    "#     },\n",
    "#     transformation_ctx=\"AmazonRedshift_node1708544284158\",\n",
    "# )\n",
    "\n",
    "# job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Schema: \n",
    "**15 Features extracted and Converted with 5458 total current records**\n",
    "- job_id VARCHAR,\n",
    "- agency VARCHAR,\n",
    "- posting_type VARCHAR,\n",
    "- number_of_positions INTEGER,\n",
    "- business_title VARCHAR,\n",
    "- civil_service_title VARCHAR,\n",
    "- level VARCHAR,\n",
    "- full_time_part_time_indicator VARCHAR,\n",
    "- career_level VARCHAR, salary_range_from DECIMAL,\n",
    "- salary_range_to DECIMAL,\n",
    "- salary_frequency VARCHAR,\n",
    "- work_location VARCHAR, \n",
    "- division_work_unit VARCHAR, \n",
    "- posting_date DATE, \n",
    "- post_until VARCHAR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda-env)",
   "language": "python",
   "name": "cuda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
